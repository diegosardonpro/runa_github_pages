import os
import re
import json
import requests
import psycopg2
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
from supabase import create_client, Client
from src.utils.logger import get_logger

# --- CONFIGURACIÓN ---
LOGGER = get_logger(__name__)
URLS_TABLE = 'urls_para_procesar'
ASSETS_TABLE = 'activos_curados'
IMAGES_OUTPUT_DIR = 'output_images'

# --- DEFINICIÓN DE LA INFRAESTRUCTURA (IaC) ---
SCHEMA_SQL = f"""
CREATE TABLE IF NOT EXISTS public.{URLS_TABLE} (
    id bigint GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    created_at timestamptz DEFAULT now() NOT NULL,
    url text NOT NULL UNIQUE,
    estado text DEFAULT 'pendiente' NOT NULL, -- pendiente, en_proceso, completado, error
    intentos smallint DEFAULT 0 NOT NULL,
    ultimo_error text
);

CREATE TABLE IF NOT EXISTS public.{ASSETS_TABLE} (
    id bigint GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    created_at timestamptz DEFAULT now() NOT NULL,
    url_original text NOT NULL,
    titulo text,
    resumen text,
    contenido_html text,
    tags text, -- Tags generados por IA, separados por comas
    ruta_imagen_local text, -- Ruta a la imagen principal descargada
    url_imagen_original text
);

ALTER TABLE public.{URLS_TABLE} ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.{ASSETS_TABLE} ENABLE ROW LEVEL SECURITY;
"""

# --- FUNCIONES DE CONFIGURACIÓN Y CONEXIÓN ---
def setup_database_schema():
    conn = None
    try:
        db_url = os.getenv('SUPABASE_URL')
        db_password = os.getenv('SUPABASE_DB_PASSWORD')
        if not db_url or not db_password: raise ValueError("Secretos de BD no encontrados.")
        
        project_ref = urlparse(db_url).hostname.split('.')[0]
        db_host = f"db.{project_ref}.supabase.co"
        conn_string = f"postgresql://postgres:{db_password}@{db_host}:5432/postgres"
        
        LOGGER.info(f"Conectando a {db_host} para configurar schema...")
        conn = psycopg2.connect(conn_string)
        cursor = conn.cursor()
        LOGGER.info("Ejecutando SQL para crear tablas si no existen...")
        cursor.execute(SCHEMA_SQL)
        conn.commit()
        LOGGER.info("Schema de la base de datos verificado/creado con éxito.")
    except Exception as e:
        LOGGER.error(f"Error al configurar el schema de la BD: {e}")
        raise
    finally:
        if conn: conn.close()

def get_supabase_client():
    url = os.getenv('SUPABASE_URL')
    key = os.getenv('SUPABASE_SERVICE_KEY')
    if not url or not key: raise ValueError("Secretos de Supabase no encontrados.")
    return create_client(url, key)

# --- FUNCIONES DE PROCESAMIENTO ---
def scrape_article_data(url):
    # ... (Implementación de scraping similar a la anterior)
    pass

def enrich_with_ai(text_content):
    # ... (Implementación de enriquecimiento con IA similar a la anterior)
    pass

def download_image(image_url, asset_id):
    # ... (Implementación de descarga de imagen similar a la anterior)
    pass

# --- LÓGICA PRINCIPAL ---
def main():
    LOGGER.info("--- Iniciando Curador de Activos Digitales v1.0 ---")
    try:
        # 1. Asegurar que la infraestructura de la BD esté lista
        setup_database_schema()
        supabase = get_supabase_client()

        # 2. Obtener lote de URLs para procesar
        LOGGER.info(f"Buscando URLs con estado 'pendiente' en la tabla '{URLS_TABLE}'...")
        response = supabase.table(URLS_TABLE).select('id, url').eq('estado', 'pendiente').limit(5).execute()
        urls_to_process = response.data

        if not urls_to_process:
            LOGGER.info("No hay nuevas URLs para procesar. Finalizando.")
            return

        LOGGER.info(f"Se encontraron {len(urls_to_process)} URLs para procesar.")
        
        # Marcar como 'en_proceso' para evitar que otros workers las tomen
        ids_in_process = [item['id'] for item in urls_to_process]
        supabase.table(URLS_TABLE).update({'estado': 'en_proceso'}).in_('id', ids_in_process).execute()

        # 3. Procesar cada URL
        for item in urls_to_process:
            url_id, url = item['id'], item['url']
            LOGGER.info(f"Procesando ID {url_id}: {url}")
            try:
                # Aquí iría la lógica completa de scrape, enrich, download
                # Por simplicidad, esta versión solo simula el proceso
                LOGGER.info("  (Simulación) Scrapeando texto e imagen...")
                LOGGER.info("  (Simulación) Analizando con IA para obtener tags...")
                LOGGER.info("  (Simulación) Descargando imagen a local...")
                
                # Insertar en la tabla de activos curados
                new_asset = {
                    'url_original': url,
                    'titulo': 'Título Extraído de ' + urlparse(url).netloc,
                    'resumen': 'Este es un resumen generado por IA...',
                    'contenido_html': '<p>Contenido HTML procesado...</p>',
                    'tags': 'simulacion, ia, curacion',
                    'ruta_imagen_local': f'images/{url_id}.jpg'
                }
                supabase.table(ASSETS_TABLE).insert(new_asset).execute()
                LOGGER.info(f"  Activo curado guardado en tabla '{ASSETS_TABLE}'.")

                # Marcar como completado
                supabase.table(URLS_TABLE).update({'estado': 'completado'}).eq('id', url_id).execute()
                LOGGER.info(f"  URL ID {url_id} marcada como 'completado'.")

            except Exception as e:
                error_message = str(e).replace('\n', ' ')
                LOGGER.error(f"  Error procesando URL ID {url_id}: {error_message}")
                supabase.table(URLS_TABLE).update({'estado': 'error', 'ultimo_error': error_message}).eq('id', url_id).execute()

        LOGGER.info("\nProceso de curación completado para este lote.")

    except Exception as e:
        LOGGER.error(f"Ha ocurrido un error fatal en el script: {e}", exc_info=True)
        exit(1)

if __name__ == "__main__":
    main()
